{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデルを使って、実際に画像から検出をします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習が進めば、実際に画像を読み込んでそこから対象物を検出できるようになります（はずです）。\n",
    "\n",
    "このノートブックでは、物体検出を体験してみます。  \n",
    "検出モデルは前のステップで学習したモデル（を検出用に変換（後述））、入力は検証用画像です。  \n",
    "また「検出がどれくらいうまくいっているか」を測るための、mAP（Mean Average Precision）という評価指標とその計算方法も紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../models/research')\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑ matplotlib の設定でコンフリクトが起きるのでエラーが出ますが、notebookの動作には支障ありません。無視してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデル（グラフデータおよびチェックポイントファイル）を、変数情報を固定化して検出に特化したモデル（フローズングラフ）に変換します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓ `model.ckpt-2000` の箇所は、実際に回したステップ数に応じて適宜修正してください（100ステップなら `model.ckpt-100`、等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sh\n",
    "env PYTHONPATH=$PYTHONPATH:../models/research:../models/research/slim \\\n",
    "    python ../models/research/object_detection/export_inference_graph.py \\\n",
    "        --input_type image_tensor \\\n",
    "        --pipeline_config_path ./ssd_mobilenet_v1_manabiya.pbtxt \\\n",
    "        --trained_checkpoint_prefix ./train/model.ckpt-2000 \\\n",
    "        --output_directory ./frozen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑ `Converted 199 variables to const ops.` と出力されればOK。  \n",
    "　警告が出る場合がありますが無視してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変換に成功すると、`./frozen/` ディレクトリ以下にいくつかのファイルが生成されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 変数設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検出で使用するいくつかの変数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# フローズングラフへのパス。これが実際に物体検出に利用されるモデルです。\n",
    "PATH_TO_FROZEN_GRAPH = './frozen/frozen_inference_graph.pb'\n",
    "\n",
    "# ラベルマップファイルへのパス（Step1 で生成したもの。出力ラベル表示時に利用）\n",
    "PATH_TO_LABELS = './label_map.pbtxt'\n",
    "\n",
    "# クラス数\n",
    "NUM_CLASSES = 36\n",
    "\n",
    "# 表示画像サイズ（Jupyter Notebook 上に検出結果を表示する際にのみ使用）\n",
    "DISPLAY_IMAGE_SIZE = (12*2, 8*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの読込"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PATH_TO_FROZEN_GRAPH` からモデルを読み込みます。TensorFlow の `tf.Graph` および `tf.GraphDef` を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラベルマップの読込"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PATH_TO_LABELS` からラベルマップを読み込みます。TF Object Detection で用意されているユーティリティ関数を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ラベルマップをそのまま読込\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "# ↑を「ディクショナリのリスト」に変換（評価時に利用）\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "# ↑をさらに「ディクショナリ（ID->（ラベルマップの1項目を表す）ディクショナリ）」に変換（検出結果表示時に利用）\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ヘルパーコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "読み込んだ画像をRGBのバイト列（3次元配列）に変換する関数を準備します。  \n",
    "検出する際には画像ファイルの生データではなくバイト列にデコードしたデータを入力とする必要があり、そのためのものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備された検証用の画像を読み込む準備をします。  \n",
    "ここでは画像ディレクトリのパスと「input_data_id と ファイル名の対応辞書」準備しているだけです。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"画像ディレクトリのパス\"\"\"\n",
    "PATH_TO_TEST_IMAGES_DIR = './data-manabiya/width1000/image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"input_data_id と ファイル名の対応辞書\"\"\"\n",
    "input_data_id_2_input_name = {\n",
    "    \"2e152501-d5d9-4026-bde0-7c28c1e773e0\":\"540782936.273833.png\",\n",
    "    \"41816be2-3f57-4be9-bfd3-d228a4c1ca69\":\"540782823.324211.png\",\n",
    "    \"418fc9c8-3ec4-49b2-8176-a8cf5376c026\":\"540782586.412328.png\",\n",
    "    \"85228d61-209f-4675-b30b-fd842b5623a1\":\"540782457.810163.png\",\n",
    "    \"b257a1e9-433d-480d-a41e-ef1dbf29d67e\":\"540782780.456003.png\",\n",
    "    \"b4c190e3-9599-4d7f-9d97-d23b09752fdc\":\"540782910.323475.png\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検出実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_inference_for_single_image()` 関数は、画像とグラフ（検出用のフローズングラフ）を受け取って、推測（検出）を実行する関数です。  \n",
    "長いのでかいつまんで仕様（概要）を説明します："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 引数：\n",
    "    + `image`: 画像データ（縦×横×RGBの3次元配列）\n",
    "    + `graph`: フローズングラフ（を読み込んだ `tf.Graph`）\n",
    "+ 処理概要：\n",
    "    1. フローズングラフから、検出結果を返すテンソル（ops）を抽出\n",
    "    2. 受け取った画像データを引き渡して推測→検出結果を取得\n",
    "    3. 数値を適宜修正（補正）して返す\n",
    "+ 戻り値：\n",
    "    + 辞書：\n",
    "        + 各キーとその値：\n",
    "            + `num_detections`: 検出数\n",
    "            + `detection_boxes`: 検出した矩形（BoundingBox）のリスト\n",
    "            + `detection_classes`: 検出したクラスのリスト\n",
    "            + `detection_scores`: 検出結果のスコアのリスト\n",
    "        + 各 `detection_～es` は `num_detections` 個の要素からなるリスト（numpy array）\n",
    "        + 各 `detection_～es` の各`n`番目の要素は互いに対応（`n`番目の矩形<->`n`番目のクラス<->`n`番目のスコア）\n",
    "        + `detection_scores` はスコアの高いものから降順で並んでいる\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                    'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                    'detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                            tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                        detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                        tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                        detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict, feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "                    'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 検出実行（評価用）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TEST_IMAGE_PATHS` から各画像を読み込んで検出結果を収集および表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_data_id と「出力結果の辞書」の対応辞書（準備）\n",
    "output_dicts_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for (input_data_id, filename) in input_data_id_2_input_name.items():\n",
    "    image_path = os.path.join(PATH_TO_TEST_IMAGES_DIR, filename)\n",
    "    image_np = load_image_into_numpy_array(Image.open(image_path))\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    output_dicts_dict[input_data_id] = output_dict\n",
    "\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks'),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=8)\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検証用画像の検出結果を利用して、mAP（Mean Average Precision）を計算します。  \n",
    "まずは、mAP（およびその周辺知識）の概要を説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Precision と Recall：\n",
    "    + Precision（適合率）：\n",
    "        + 「正しい」と判定されたもののうち、実際に正しかったものの割合\n",
    "        + Precision が高いほど、『誤検出』が少ない\n",
    "    + Recall（再現率）：\n",
    "        + 実際に正しいもののうち、「正しい」と判定されたものの割合\n",
    "        + Recall が高いほど、『検出漏れ』が少ない\n",
    "+ AP（Average Precision：平均適合率）：\n",
    "    + 各 Recall に対する Precision の平均を取ったもの\n",
    "+ mAP（Mean Average Precision）：\n",
    "    + 各クラスごとに算出した AP の平均を取ったもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import config_util\n",
    "from object_detection import evaluator as od_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configs = config_util.get_configs_from_pipeline_file('./ssd_mobilenet_v1_manabiya.pbtxt')\n",
    "eval_config = configs['eval_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = od_evaluator.get_evaluators(eval_config, categories)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bounding_box(coordinates):\n",
    "    _x = 0\n",
    "    _y = 1\n",
    "    _xs = [t[_x] for t in coordinates]\n",
    "    _ys = [t[_y] for t in coordinates]\n",
    "    min_x, min_y = min(_xs), min(_ys)\n",
    "    max_x, max_y = max(_xs), max(_ys)\n",
    "    return (min_x, min_y, max_x, max_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_text_2_index = {item['name']: item['id'] for item in categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data_id_2_gtbs = {}\n",
    "input_data_id_2_classes = {}\n",
    "for input_data_id in input_data_id_2_input_name.keys():\n",
    "    annotation_json_path = os.path.join(\"./data-manabiya/width1000/json\", \"{}.json\".format(input_data_id))\n",
    "    json_dict = json.load(open(annotation_json_path, 'r'))\n",
    "    # input_data_id = json_dict[\"input_data_id\"]\n",
    "    # assert input_data_id == json_dict[\"input_data_id\"]\n",
    "    # 画像サイズは決め打ち\n",
    "    width,height = 1000.0, 750.0\n",
    "    gtbs = []\n",
    "    classes = []\n",
    "    for annotation in json_dict[\"detail\"]:\n",
    "        flat_coordinates = annotation[\"data\"].split(\",\")\n",
    "        xy_coordinates = [(int(x),int(y)) for (x,y) in zip(*[iter(flat_coordinates)]*2)]\n",
    "        min_x, min_y, max_x, max_y = bounding_box(xy_coordinates)\n",
    "        gtbs.append([min_y / height, min_x / width, max_y / height, max_x / width])\n",
    "        class_text = str(annotation[\"additional_data_list\"][0][\"choice\"])\n",
    "        classes.append(class_text_2_index[class_text])\n",
    "    input_data_id_2_gtbs[input_data_id] = np.array(gtbs)\n",
    "    input_data_id_2_classes[input_data_id] = np.array(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑警告が出ることがありますが気にしないでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for input_data_id in output_dicts_dict.keys():\n",
    "    evaluator.add_single_ground_truth_image_info(input_data_id, {\n",
    "        \"groundtruth_boxes\": input_data_id_2_gtbs[input_data_id],\n",
    "        \"groundtruth_classes\": input_data_id_2_classes[input_data_id]\n",
    "    })\n",
    "    evaluator.add_single_detected_image_info(input_data_id, output_dicts_dict[input_data_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑警告が出ることがありますが気にしないでください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_output_dict = evaluator.evaluate()\n",
    "eval_output_dict"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
