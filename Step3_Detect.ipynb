{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデルを使って、実際に画像から検出をします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'../models/research')\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑ matplotlib の設定でコンフリクトが起きるのでエラーが出ますが、notebookの動作には支障ありません。無視してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデル（グラフデータおよびチェックポイントファイル）を、変数情報を固定化して検出に特化したモデル（フローズングラフ）に変換します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↓ `model.ckpt-2000` の箇所は、実際に回したステップ数に応じて適宜修正してください（100ステップなら `model.ckpt-100`、等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sh\n",
    "env PYTHONPATH=$PYTHONPATH:../models/research:../models/research/slim \\\n",
    "    python ../models/research/object_detection/export_inference_graph.py \\\n",
    "        --input_type image_tensor \\\n",
    "        --pipeline_config_path ./ssd_mobilenet_v1_manabiya.pbtxt \\\n",
    "        --trained_checkpoint_prefix ./train/model.ckpt-2000 \\\n",
    "        --output_directory ./frozen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑ `Converted 199 variables to const ops.` と出力されればOK。  \n",
    "　警告が出る場合がありますが無視してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変換に成功すると、`./frozen/` ディレクトリ以下にいくつかのファイルが生成されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 変数設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検出で使用するいくつかの変数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# フローズングラフへのパス。これが実際に物体検出に利用されるモデルです。\n",
    "PATH_TO_FROZEN_GRAPH = './frozen/frozen_inference_graph.pb'\n",
    "\n",
    "# ラベルマップファイルへのパス（Step1 で生成したもの。出力ラベル表示時に利用）\n",
    "PATH_TO_LABELS = './label_map.pbtxt'\n",
    "\n",
    "# クラス数\n",
    "NUM_CLASSES = 36\n",
    "\n",
    "# 表示画像サイズ（Jupyter Notebook 上に検出結果を表示する際にのみ使用）\n",
    "DISPLAY_IMAGE_SIZE = (12*2, 8*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの読込"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PATH_TO_FROZEN_GRAPH` からモデルを読み込みます。TensorFlow の `tf.Graph` および `tf.GraphDef` を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラベルマップの読込"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PATH_TO_LABELS` からラベルマップを読み込みます。TF Object Detection で用意されているユーティリティ関数を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ラベルマップをそのまま読込\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "# ↑を「ディクショナリのリスト」に変換（評価時に利用）\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "# ↑をさらに「ディクショナリ（ID->（ラベルマップの1項目を表す）ディクショナリ）」に変換（検出結果表示時に利用）\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ヘルパーコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "読み込んだ画像をRGBのバイト列（3次元配列）に変換する関数を準備します。  \n",
    "検出する際には画像ファイルの生データではなくバイト列にデコードしたデータを入力とする必要があり、そのためのものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "準備された検証用の画像を読み込む準備をします。  \n",
    "ここでは画像のパスをリストで用意しているだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_TEST_IMAGES_DIR = './data-manabiya/width1000/image'\n",
    "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, '*.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検出実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_inference_for_single_image()` 関数は、画像とグラフ（検出用のフローズングラフ）を受け取って、推測（検出）を実行する関数です。  \n",
    "長いのでかいつまんで仕様（概要）を説明します："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 引数：\n",
    "    + `image`: 画像データ（縦×横×RGBの3次元配列）\n",
    "    + `graph`: フローズングラフ（を読み込んだ `tf.Graph`）\n",
    "+ 処理概要：\n",
    "    1. フローズングラフから、検出結果を返すテンソル（ops）を抽出\n",
    "    2. 受け取った画像データを引き渡して推測→検出結果を取得\n",
    "    3. 数値を適宜修正（補正）して返す\n",
    "+ 戻り値：\n",
    "    + 辞書：\n",
    "        + 各キーとその値：\n",
    "            + `num_detections`: 検出数\n",
    "            + `detection_boxes`: 検出した矩形（BoundingBox）のリスト\n",
    "            + `detection_classes`: 検出したクラスのリスト\n",
    "            + `detection_scores`: 検出結果のスコアのリスト\n",
    "        + 各 `detection_～es` は `num_detections` 個の要素からなるリスト（numpy array）\n",
    "        + 各 `detection_～es` の各`n`番目の要素は互いに対応（`n`番目の矩形<->`n`番目のクラス<->`n`番目のスコア）\n",
    "        + `detection_scores` はスコアの高いものから降順で並んでいる\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                    'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                    'detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                            tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                        detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                        tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                        detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict, feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "                    'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TEST_IMAGE_PATHS` から各画像を読み込んで検出結果を表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    image = Image.open(image_path)\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            output_dict['detection_boxes'],\n",
    "            output_dict['detection_classes'],\n",
    "            output_dict['detection_scores'],\n",
    "            category_index,\n",
    "            instance_masks=output_dict.get('detection_masks'),\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=8)\n",
    "    plt.figure(figsize=DISPLAY_IMAGE_SIZE)\n",
    "    plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
